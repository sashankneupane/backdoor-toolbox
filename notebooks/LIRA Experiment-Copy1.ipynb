{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e99bb8",
   "metadata": {},
   "source": [
    "## LIRA original paper experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e51928b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from backdoor.attacks import LiraAttack\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a20560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Get all the datasets used in the original paper (MNIST, CIFAR10, GTSRB, T-ImageNet)\n",
    "\n",
    "# all my datasets are in '/data/' folder\n",
    "root = '/data/'\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "mnist_trainset = datasets.MNIST(root=root, train=True, download=True, transform=mnist_transform)\n",
    "mnist_testset = datasets.MNIST(root=root, train=False, download=True, transform=mnist_transform)\n",
    "\n",
    "# CIFAR10 dataset\n",
    "cifar10_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                        (0.247, 0.243, 0.261))\n",
    "])\n",
    "cifar10_trainset = datasets.CIFAR10(root=root, train=True, download=True, transform=cifar10_transform)\n",
    "cifar10_testset = datasets.CIFAR10(root=root, train=False, download=True, transform=cifar10_transform)\n",
    "\n",
    "# GTSRB dataset\n",
    "gtsrb_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3401, 0.3120, 0.3212), (0.2725, 0.2609, 0.2669))\n",
    "])\n",
    "gtsrb_trainset = datasets.ImageFolder(root=root+'gtsrb/GTSRB/Training', transform=gtsrb_transform)\n",
    "gtsrb_testset = datasets.ImageFolder(root=root+'gtsrb/GTSRB/Final_Test', transform=gtsrb_transform)\n",
    "# Tiny ImageNet dataset\n",
    "tinyimagenet_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "tinyimagenet_trainset = datasets.ImageFolder(root=root+'tiny-imagenet-200/train', transform=tinyimagenet_transform)\n",
    "tinyimagenet_testset = datasets.ImageFolder(root=root+'tiny-imagenet-200/val', transform=tinyimagenet_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199275a",
   "metadata": {},
   "source": [
    "## LIRA attack on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e338b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model used in the original code\n",
    "class MNISTBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(MNISTBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.ind = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv1(torch.relu(self.bn1(x)))\n",
    "\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # 14\n",
    "            nn.ReLU(),\n",
    "            MNISTBlock(32, 64, stride=2),  # 7\n",
    "            MNISTBlock(64, 64, stride=2),  # 4\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class MNISTAutoencoder(nn.Module):\n",
    "    \"\"\"The generator of backdoor trigger on MNIST.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 64, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # b, 8, 2, 2\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 128, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6767e3fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage I LIRA attack with alternating optimization\n",
      "\n",
      "Epoch 1/20\t|\tClassifier Loss: 0.6223769524791983\t|\tTrigger Loss: 0.45985645783491796\n",
      "Test Accuracy: 0.96\n",
      "Attack success rate: 0.0022560631697687537\n",
      "\n",
      "\n",
      "Epoch 2/20\t|\tClassifier Loss: 0.04488226400017103\t|\tTrigger Loss: 0.004129834094459239\n",
      "Test Accuracy: 0.9817\n",
      "Attack success rate: 0.0010152284263959391\n",
      "\n",
      "\n",
      "Epoch 3/20\t|\tClassifier Loss: 0.03695561706638539\t|\tTrigger Loss: 0.009449470602852157\n",
      "Test Accuracy: 0.9809\n",
      "Attack success rate: 0.003045685279187817\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     25\u001b[0m mnist_trigger_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(\n\u001b[1;32m     26\u001b[0m     mnist_trigger\u001b[38;5;241m.\u001b[39mparameters(), \n\u001b[1;32m     27\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     31\u001b[0m mnist_lira_attack \u001b[38;5;241m=\u001b[39m LiraAttack(\n\u001b[1;32m     32\u001b[0m     device,\n\u001b[1;32m     33\u001b[0m     mnist_classifier,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m \u001b[43mmnist_lira_attack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmnist_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetune_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmnist_finetune_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmnist_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrigger_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmnist_trigger_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetune_test_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetune_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmnist_finetune_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetune_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmnist_finetune_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/backdoor-toolbox/venv/lib/python3.10/site-packages/backdoor/attacks/lira.py:297\u001b[0m, in \u001b[0;36mLiraAttack.attack\u001b[0;34m(self, epochs, finetune_epochs, optimizer, trigger_optimizer, finetune_optimizer, finetune_scheduler, loss_function, finetune_loss_function, update_trigger_epochs, alpha, finetune_alpha, eps, finetune_test_eps, eval_every, num_workers)\u001b[0m\n\u001b[1;32m    294\u001b[0m temp_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# We only want to update the classifier parameters from this loss\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Now that we have updated the classifier, we can calculate the trigger loss using this updated classifier\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/backdoor-toolbox/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/backdoor-toolbox/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Desktop/backdoor-toolbox/venv/lib/python3.10/site-packages/torch/optim/sgd.py:76\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 76\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/Desktop/backdoor-toolbox/venv/lib/python3.10/site-packages/torch/optim/sgd.py:222\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 222\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/backdoor-toolbox/venv/lib/python3.10/site-packages/torch/optim/sgd.py:306\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_states_with_momentum_buffer:\n\u001b[1;32m    305\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(bufs, momentum)\n\u001b[0;32m--> 306\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdampening\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     bufs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hyperparameters based on the original paper\n",
    "mnist_epochs = 10\n",
    "mnist_finetune_epochs = 10\n",
    "\n",
    "mnist_classifier = MNISTClassifier().to(device)\n",
    "mnist_trigger = MNISTAutoencoder().to(device)\n",
    "\n",
    "# create optimizers and schedulers for the attack\n",
    "mnist_optimizer = optim.SGD(\n",
    "    mnist_classifier.parameters(), \n",
    "    lr=0.01, \n",
    "    momentum=0.9\n",
    ")\n",
    "mnist_finetune_optimizer = optim.SGD(\n",
    "    mnist_classifier.parameters(), \n",
    "    lr=0.01, \n",
    "    momentum=0.9, \n",
    "    weight_decay=5e-4\n",
    ")\n",
    "mnist_finetune_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "    mnist_finetune_optimizer, \n",
    "    milestones=[10,20,30,40],\n",
    "    gamma=0.1\n",
    ")\n",
    "mnist_trigger_optimizer = optim.SGD(\n",
    "    mnist_trigger.parameters(), \n",
    "    lr=0.0001\n",
    ")\n",
    "\n",
    "\n",
    "mnist_lira_attack = LiraAttack(\n",
    "    device,\n",
    "    mnist_classifier,\n",
    "    mnist_trigger,\n",
    "    mnist_trainset,\n",
    "    mnist_testset,\n",
    "    target_class=1, # trigger class\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "mnist_lira_attack.attack(\n",
    "    epochs=mnist_epochs,\n",
    "    finetune_epochs=mnist_finetune_epochs,\n",
    "    optimizer=mnist_optimizer,\n",
    "    trigger_optimizer=mnist_trigger_optimizer,\n",
    "    finetune_test_eps=0.01,\n",
    "    finetune_optimizer=mnist_finetune_optimizer,\n",
    "    finetune_scheduler=mnist_finetune_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_lira_attack.save_model('../models/mnist_lira_attack')\n",
    "trigger_model = mnist_lira_attack.trigger_model\n",
    "torch.save(trigger_model.state_dict(), '../models/mnist_lira_trigger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ef3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(triggerlosses, classifierlosses, stageIenditr, trainlosses, stageIendepoch):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    axs[0].plot(triggerlosses)\n",
    "    axs[0].set_title(\"Trigger Loss during Phase I\")\n",
    "\n",
    "    axs[1].plot(classifierlosses)\n",
    "    axs[1].axvline(x=stageIenditr, color='r', linestyle='--')\n",
    "    axs[1].set_title(\"Classifier Loss during both Phases\")\n",
    "\n",
    "    # Plotting third loss\n",
    "    axs[2].plot(trainlosses)\n",
    "    axs[2].axvline(x=stageIendepoch, color='r', linestyle='--')\n",
    "    axs[2].set_title(\"Average Loss after every epoch\")\n",
    "\n",
    "    # Adjusting the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "triggerlosses = mnist_lira_attack.triggerlosses\n",
    "classifierlosses = mnist_lira_attack.classifierlosses\n",
    "trainlosses = mnist_lira_attack.trainlosses\n",
    "plot_losses(triggerlosses, classifierlosses, 15*60000/128, trainlosses, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc229fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def apply_trigger(dataset, trigger_model):\n",
    "#     # Apply trigger model to the dataset\n",
    "#     transformed_dataset = []\n",
    "#     for sample in dataset:\n",
    "#         image, label, poisoned_label = sample\n",
    "#         image = image.to(device)\n",
    "#         image += trigger_model(image.unsqueeze(0)).squeeze() * 0.01 # eps\n",
    "#         transformed_dataset.append((image, label, poisoned_label))\n",
    "#     return transformed_dataset\n",
    "\n",
    "# def plot(dataset, model, num):\n",
    "#     # Plot 25 MNIST images\n",
    "#     fig, axs = plt.subplots(num, num, figsize=(5, 5))\n",
    "#     fig.tight_layout()\n",
    "    \n",
    "#     for j in range(num*num):\n",
    "#         ax = axs[j // num, j % num]\n",
    "#         ax.imshow(dataset[j][0].squeeze().detach().cpu(), cmap='gray')\n",
    "#         ax.axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Predict labels using the model\n",
    "#     predicted_labels = get_outputs(dataset, model)\n",
    "#     print(\"Labels predicted by the model:\")\n",
    "#     print(predicted_labels)\n",
    "\n",
    "# def get_outputs(dataset, model):\n",
    "#     outputs = []\n",
    "#     for sample in dataset:\n",
    "#         image, _, _ = sample\n",
    "#         image = image.unsqueeze(0).to(device)\n",
    "#         output = model(image)\n",
    "#         predicted_label = torch.argmax(output, dim=1)\n",
    "#         outputs.append(predicted_label.item())\n",
    "#     return outputs\n",
    "    \n",
    "# lira_model = mnist_lira_attack.classifier\n",
    "# trigger_model = mnist_lira_attack.trigger_model\n",
    "# poisoned_mnist_testset = mnist_lira_attack.get_poisoned_testset()\n",
    "\n",
    "# num = 5\n",
    "# mnist_subset = [poisoned_mnist_testset[i] for i in range(num*num)]\n",
    "# transformed_mnist_subset = apply_trigger(mnist_subset, trigger_model)\n",
    "\n",
    "# print('Normal Images')\n",
    "# plot(mnist_subset, lira_model, 5)\n",
    "# print()\n",
    "# print('Poisoned Images')\n",
    "# plot(transformed_mnist_subset, lira_model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38129641",
   "metadata": {},
   "source": [
    "## LIRA Attack on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1578c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "cfg = {\n",
    "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"VGG16\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
    "    \"VGG19\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"],\n",
    "}\n",
    "\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, num_classes=10, feature_dim=512):\n",
    "        \"\"\"\n",
    "        for image size 32, feature_dim = 512\n",
    "        for other sizes, feature_dim = 512 * (size//32)**2\n",
    "        \"\"\"\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == \"M\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [\n",
    "                    nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm2d(x),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                ]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"The generator of backdoor trigger on CIFAR10.\"\"\"\n",
    "    def __init__(self, out_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.AvgPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "        self.conv_last = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channel, 1),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "\n",
    "        x = self.dconv_down4(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        out = F.tanh(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a2e878",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage I LIRA attack with alternating optimization\n",
      "\n",
      "Epoch 1/100\t|\tClassifier Loss: 1.1275449306763652\t|\tTrigger Loss: 0.6380978882160333\n",
      "Test Accuracy: 0.3472\n",
      "Attack success rate: 0.6972222222222222\n",
      "\n",
      "\n",
      "Epoch 2/100\t|\tClassifier Loss: 0.9305454712084797\t|\tTrigger Loss: 0.6342584800232401\n",
      "Test Accuracy: 0.3583\n",
      "Attack success rate: 0.694\n",
      "\n",
      "\n",
      "Epoch 3/100\t|\tClassifier Loss: 0.8424044441993889\t|\tTrigger Loss: 0.6318282132106059\n",
      "Test Accuracy: 0.3007\n",
      "Attack success rate: 0.7626666666666667\n",
      "\n",
      "\n",
      "Epoch 4/100\t|\tClassifier Loss: 0.7723663156599645\t|\tTrigger Loss: 0.6329417843038164\n",
      "Test Accuracy: 0.2608\n",
      "Attack success rate: 0.8023333333333333\n",
      "\n",
      "\n",
      "Epoch 5/100\t|\tClassifier Loss: 0.7167808892172011\t|\tTrigger Loss: 0.6323264470643095\n",
      "Test Accuracy: 0.3011\n",
      "Attack success rate: 0.7632222222222222\n",
      "\n",
      "\n",
      "Epoch 6/100\t|\tClassifier Loss: 0.676462191297575\t|\tTrigger Loss: 0.6322338225896401\n",
      "Test Accuracy: 0.3451\n",
      "Attack success rate: 0.7065555555555556\n",
      "\n",
      "\n",
      "Epoch 7/100\t|\tClassifier Loss: 0.6515867922007276\t|\tTrigger Loss: 0.631663502558418\n",
      "Test Accuracy: 0.6493\n",
      "Attack success rate: 0.32766666666666666\n",
      "\n",
      "\n",
      "Epoch 8/100\t|\tClassifier Loss: 0.6398429555051467\t|\tTrigger Loss: 0.631068194689958\n",
      "Test Accuracy: 0.1086\n",
      "Attack success rate: 0.9896666666666667\n",
      "\n",
      "\n",
      "Epoch 9/100\t|\tClassifier Loss: 0.6329334559647933\t|\tTrigger Loss: 0.626132291982241\n",
      "Test Accuracy: 0.4974\n",
      "Attack success rate: 0.5216666666666666\n",
      "\n",
      "\n",
      "Epoch 10/100\t|\tClassifier Loss: 0.6441473361781186\t|\tTrigger Loss: 0.5998845597743379\n",
      "Test Accuracy: 0.2678\n",
      "Attack success rate: 0.7952222222222223\n",
      "\n",
      "\n",
      "Epoch 11/100\t|\tClassifier Loss: 0.6611464266734355\t|\tTrigger Loss: 0.2586928164512586\n",
      "Test Accuracy: 0.1016\n",
      "Attack success rate: 0.9981111111111111\n",
      "\n",
      "\n",
      "Epoch 12/100\t|\tClassifier Loss: 0.20772011043584865\t|\tTrigger Loss: 0.039140113964690255\n",
      "Test Accuracy: 0.1707\n",
      "Attack success rate: 0.914\n",
      "\n",
      "\n",
      "Epoch 13/100\t|\tClassifier Loss: 0.08035889226952782\t|\tTrigger Loss: 0.009640814473305394\n",
      "Test Accuracy: 0.1007\n",
      "Attack success rate: 0.9992222222222222\n",
      "\n",
      "\n",
      "Epoch 14/100\t|\tClassifier Loss: 0.08773995178234775\t|\tTrigger Loss: 0.024022728981632636\n",
      "Test Accuracy: 0.1002\n",
      "Attack success rate: 0.9997777777777778\n",
      "\n",
      "\n",
      "Epoch 15/100\t|\tClassifier Loss: 0.03074601740526307\t|\tTrigger Loss: 0.0032479678981477715\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 14 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 16/100\t|\tClassifier Loss: 0.022836358188782508\t|\tTrigger Loss: 0.0033918899564865917\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 15 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 17/100\t|\tClassifier Loss: 0.011227810793482434\t|\tTrigger Loss: 0.0009140828535244212\n",
      "Test Accuracy: 0.1068\n",
      "Attack success rate: 0.992\n",
      "\n",
      "\n",
      "Epoch 18/100\t|\tClassifier Loss: 0.00324563444105258\t|\tTrigger Loss: 0.00042541153769536804\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 17 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 19/100\t|\tClassifier Loss: 0.0027524361100248504\t|\tTrigger Loss: 0.0008543910465986966\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 18 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 20/100\t|\tClassifier Loss: 0.0008915360563767307\t|\tTrigger Loss: 0.0001684637575522756\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 19 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 21/100\t|\tClassifier Loss: 0.02293832817374777\t|\tTrigger Loss: 0.00887328017726944\n",
      "Test Accuracy: 0.1095\n",
      "Attack success rate: 0.9892222222222222\n",
      "\n",
      "\n",
      "Epoch 22/100\t|\tClassifier Loss: 0.001463515278583308\t|\tTrigger Loss: 0.000181461795323451\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 21 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 23/100\t|\tClassifier Loss: 0.0006426161001974573\t|\tTrigger Loss: 0.00019879667837545677\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 22 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 24/100\t|\tClassifier Loss: 0.0007425508344063031\t|\tTrigger Loss: 0.00022475413136559467\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 23 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 25/100\t|\tClassifier Loss: 0.0003204377949341064\t|\tTrigger Loss: 0.00011526089984979238\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 24 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 26/100\t|\tClassifier Loss: 0.00014722064565772655\t|\tTrigger Loss: 3.517422928150213e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 25 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 27/100\t|\tClassifier Loss: 0.00010819163107046331\t|\tTrigger Loss: 5.682376061051017e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 26 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 28/100\t|\tClassifier Loss: 0.00010290709142724094\t|\tTrigger Loss: 5.880294288316728e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 27 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 29/100\t|\tClassifier Loss: 7.106515479155669e-05\t|\tTrigger Loss: 3.940768896152122e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 28 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 30/100\t|\tClassifier Loss: 7.206419184901379e-05\t|\tTrigger Loss: 4.126128448033289e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 29 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 31/100\t|\tClassifier Loss: 5.7055304571867105e-05\t|\tTrigger Loss: 3.3704873087047376e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 30 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 32/100\t|\tClassifier Loss: 6.606258642923212e-05\t|\tTrigger Loss: 4.9591330078550606e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 31 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 33/100\t|\tClassifier Loss: 7.274049337405455e-05\t|\tTrigger Loss: 3.087849037724097e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 32 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 34/100\t|\tClassifier Loss: 4.112710304615089e-05\t|\tTrigger Loss: 1.657546527388853e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 33 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 35/100\t|\tClassifier Loss: 0.00013697370329510377\t|\tTrigger Loss: 0.00011163993409649565\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 34 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 36/100\t|\tClassifier Loss: 4.7429223382549684e-05\t|\tTrigger Loss: 2.7295953866200363e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 35 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 37/100\t|\tClassifier Loss: 3.504121892940225e-05\t|\tTrigger Loss: 8.79242459492132e-06\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 36 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 38/100\t|\tClassifier Loss: 3.9515166224966073e-05\t|\tTrigger Loss: 1.9345749678617814e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 37 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 39/100\t|\tClassifier Loss: 0.000109224024814336\t|\tTrigger Loss: 7.741132604362314e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 38 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 40/100\t|\tClassifier Loss: 3.3793276563064076e-05\t|\tTrigger Loss: 2.27569088011246e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 39 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 41/100\t|\tClassifier Loss: 9.216692825431934e-05\t|\tTrigger Loss: 7.761446674142081e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 40 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 42/100\t|\tClassifier Loss: 7.651716277655454e-05\t|\tTrigger Loss: 2.1899212718190594e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 41 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 43/100\t|\tClassifier Loss: 4.831931799128277e-05\t|\tTrigger Loss: 5.465994381702704e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 42 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 44/100\t|\tClassifier Loss: 2.7039979085600263e-05\t|\tTrigger Loss: 9.600051897604206e-06\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 43 with ASR 1.0, so early stopping Stage I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/100\t|\tClassifier Loss: 4.6106206138128026e-05\t|\tTrigger Loss: 3.9240741148750137e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 44 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 46/100\t|\tClassifier Loss: 2.4638086414925614e-05\t|\tTrigger Loss: 3.894534276075806e-06\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 45 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 47/100\t|\tClassifier Loss: 2.4878539167899998e-05\t|\tTrigger Loss: 6.296562738427305e-06\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 46 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 48/100\t|\tClassifier Loss: 3.3020407281211e-05\t|\tTrigger Loss: 2.0688400780645974e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 47 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 49/100\t|\tClassifier Loss: 1.8181376211676653e-05\t|\tTrigger Loss: 4.590727068098506e-06\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 48 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Epoch 50/100\t|\tClassifier Loss: 2.2819840660536788e-05\t|\tTrigger Loss: 1.273991512703897e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "Attack Successful at epoch 49 with ASR 1.0, so early stopping Stage I\n",
      "\n",
      "Stage II LIRA attack with backdoor finetuning\n",
      "\n",
      "Epoch 51/100\t|\tClassifier Loss: 2.3060219576381525e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 52/100\t|\tClassifier Loss: 4.6453942030710054e-05\n",
      "Test Accuracy: 0.1053\n",
      "Attack success rate: 0.9941111111111111\n",
      "\n",
      "\n",
      "Epoch 53/100\t|\tClassifier Loss: 2.5484138208019226e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 54/100\t|\tClassifier Loss: 1.5490652695798023e-05\n",
      "Test Accuracy: 0.1162\n",
      "Attack success rate: 0.982\n",
      "\n",
      "\n",
      "Epoch 55/100\t|\tClassifier Loss: 1.7599352936788348e-05\n",
      "Test Accuracy: 0.1033\n",
      "Attack success rate: 0.9963333333333333\n",
      "\n",
      "\n",
      "Epoch 56/100\t|\tClassifier Loss: 5.341493914745843e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 57/100\t|\tClassifier Loss: 4.162310779705263e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 58/100\t|\tClassifier Loss: 3.4463083813273947e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 59/100\t|\tClassifier Loss: 1.7036998425141053e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 60/100\t|\tClassifier Loss: 1.80613021770058e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 61/100\t|\tClassifier Loss: 2.6600454873896104e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 62/100\t|\tClassifier Loss: 3.717180081856316e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 63/100\t|\tClassifier Loss: 1.5939542759496107e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 64/100\t|\tClassifier Loss: 1.4532891834192351e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 65/100\t|\tClassifier Loss: 1.6485971098662565e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 66/100\t|\tClassifier Loss: 0.04419770985035674\n",
      "Test Accuracy: 0.1354\n",
      "Attack success rate: 0.9588888888888889\n",
      "\n",
      "\n",
      "Epoch 67/100\t|\tClassifier Loss: 0.008123313659242928\n",
      "Test Accuracy: 0.5584\n",
      "Attack success rate: 0.45\n",
      "\n",
      "\n",
      "Epoch 68/100\t|\tClassifier Loss: 0.003168261986348809\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 69/100\t|\tClassifier Loss: 0.0011095123196527546\n",
      "Test Accuracy: 0.101\n",
      "Attack success rate: 0.9987777777777778\n",
      "\n",
      "\n",
      "Epoch 70/100\t|\tClassifier Loss: 0.000303188867154954\n",
      "Test Accuracy: 0.2991\n",
      "Attack success rate: 0.7666666666666667\n",
      "\n",
      "\n",
      "Epoch 71/100\t|\tClassifier Loss: 0.0007154284835276652\n",
      "Test Accuracy: 0.1694\n",
      "Attack success rate: 0.9205555555555556\n",
      "\n",
      "\n",
      "Epoch 72/100\t|\tClassifier Loss: 0.00022406715116047104\n",
      "Test Accuracy: 0.1005\n",
      "Attack success rate: 0.9994444444444445\n",
      "\n",
      "\n",
      "Epoch 73/100\t|\tClassifier Loss: 0.00015309384027130507\n",
      "Test Accuracy: 0.1016\n",
      "Attack success rate: 0.9982222222222222\n",
      "\n",
      "\n",
      "Epoch 74/100\t|\tClassifier Loss: 0.00012055186136394067\n",
      "Test Accuracy: 0.1017\n",
      "Attack success rate: 0.998\n",
      "\n",
      "\n",
      "Epoch 75/100\t|\tClassifier Loss: 8.123047708119217e-05\n",
      "Test Accuracy: 0.1026\n",
      "Attack success rate: 0.997\n",
      "\n",
      "\n",
      "Epoch 76/100\t|\tClassifier Loss: 6.926604015354635e-05\n",
      "Test Accuracy: 0.1001\n",
      "Attack success rate: 0.9998888888888889\n",
      "\n",
      "\n",
      "Epoch 77/100\t|\tClassifier Loss: 8.460229749166001e-05\n",
      "Test Accuracy: 0.1013\n",
      "Attack success rate: 0.9985555555555555\n",
      "\n",
      "\n",
      "Epoch 78/100\t|\tClassifier Loss: 0.00010256725513465096\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 79/100\t|\tClassifier Loss: 0.00011060620828056531\n",
      "Test Accuracy: 0.1001\n",
      "Attack success rate: 0.9998888888888889\n",
      "\n",
      "\n",
      "Epoch 80/100\t|\tClassifier Loss: 0.0004446939429038109\n",
      "Test Accuracy: 0.1042\n",
      "Attack success rate: 0.9952222222222222\n",
      "\n",
      "\n",
      "Epoch 81/100\t|\tClassifier Loss: 0.000574924740425898\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 82/100\t|\tClassifier Loss: 0.00020247411828658678\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 83/100\t|\tClassifier Loss: 7.347582805147709e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 84/100\t|\tClassifier Loss: 6.28134505718907e-05\n",
      "Test Accuracy: 0.173\n",
      "Attack success rate: 0.9165555555555556\n",
      "\n",
      "\n",
      "Epoch 85/100\t|\tClassifier Loss: 5.9638930569377415e-05\n",
      "Test Accuracy: 0.1001\n",
      "Attack success rate: 0.9998888888888889\n",
      "\n",
      "\n",
      "Epoch 86/100\t|\tClassifier Loss: 4.912278334946477e-05\n",
      "Test Accuracy: 0.1035\n",
      "Attack success rate: 0.996\n",
      "\n",
      "\n",
      "Epoch 87/100\t|\tClassifier Loss: 5.6509587281066616e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 88/100\t|\tClassifier Loss: 5.2289395061194235e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 89/100\t|\tClassifier Loss: 4.5540655098898184e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 90/100\t|\tClassifier Loss: 3.7179969301301344e-05\n",
      "Test Accuracy: 0.1009\n",
      "Attack success rate: 0.999\n",
      "\n",
      "\n",
      "Epoch 91/100\t|\tClassifier Loss: 4.030890614608313e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 92/100\t|\tClassifier Loss: 7.566765620270619e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 93/100\t|\tClassifier Loss: 4.711072888731322e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 94/100\t|\tClassifier Loss: 3.638456732542902e-05\n",
      "Test Accuracy: 0.1001\n",
      "Attack success rate: 0.9998888888888889\n",
      "\n",
      "\n",
      "Epoch 95/100\t|\tClassifier Loss: 8.609032191512117e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 96/100\t|\tClassifier Loss: 3.9453353863214694e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 97/100\t|\tClassifier Loss: 7.1921392825289e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 98/100\t|\tClassifier Loss: 7.765453754663867e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 99/100\t|\tClassifier Loss: 3.233321731583957e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n",
      "\n",
      "Epoch 100/100\t|\tClassifier Loss: 5.3799908992214274e-05\n",
      "Test Accuracy: 0.1\n",
      "Attack success rate: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters based on the original paper\n",
    "cifar10_epochs = 50\n",
    "cifar10_finetune_epochs = 50\n",
    "cifar10_eps = 0.01 # to preserve stealthiness of the data\n",
    "\n",
    "cifar10_classifier = VGG('VGG11', num_classes=10).to(device)\n",
    "cifar10_trigger = UNet(3).to(device)\n",
    "\n",
    "cifar10_optimizer = optim.SGD(\n",
    "    cifar10_classifier.parameters(), \n",
    "    lr=1e-2, \n",
    "    momentum=0.9\n",
    ")\n",
    "cifar10_trigger_optimizer = optim.SGD(\n",
    "    cifar10_trigger.parameters(), \n",
    "    lr=1e-4\n",
    ")\n",
    "cifar10_finetune_optimizer = optim.SGD(\n",
    "    cifar10_classifier.parameters(),\n",
    "    lr=1e-2,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4,\n",
    ")\n",
    "cifar10_finetune_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "    cifar10_finetune_optimizer,\n",
    "    milestones=[50,100,150,200],\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "cifar10_lira_attack = LiraAttack(\n",
    "    device,\n",
    "    cifar10_classifier,\n",
    "    cifar10_trigger,\n",
    "    cifar10_trainset,\n",
    "    cifar10_testset,\n",
    "    target_class=1, # trigger class\n",
    "    batch_size=128,\n",
    ")\n",
    "\n",
    "cifar10_lira_attack.attack(\n",
    "    epochs=cifar10_epochs,\n",
    "    finetune_epochs=cifar10_finetune_epochs,\n",
    "    optimizer=cifar10_optimizer,\n",
    "    trigger_optimizer=cifar10_trigger_optimizer,\n",
    "    finetune_optimizer=cifar10_finetune_optimizer,\n",
    "    finetune_scheduler=cifar10_finetune_scheduler,\n",
    "    eps=cifar10_eps,\n",
    "    finetune_test_eps=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0acee9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LiraAttack' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(predicted_label\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m---> 35\u001b[0m cifar10_lira_model \u001b[38;5;241m=\u001b[39m \u001b[43mcifar10_lira_attack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m     36\u001b[0m cifar10_trigger_model \u001b[38;5;241m=\u001b[39m cifar10_lira_attack\u001b[38;5;241m.\u001b[39mtrigger_model\n\u001b[1;32m     37\u001b[0m poisoned_cifar10_testset \u001b[38;5;241m=\u001b[39m cifar10_lira_attack\u001b[38;5;241m.\u001b[39mget_poisoned_testset()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LiraAttack' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "def cifar10_plot(dataset, model, num):\n",
    "    # Plot 25 images\n",
    "    fig, axs = plt.subplots(num, num, figsize=(5, 5))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    for j in range(num*num):\n",
    "        ax = axs[j // num, j % num]\n",
    "        img = torch.tensor(dataset[j][0])\n",
    "        img = img.permute(1,2,0).clone().detach().squeeze().cpu().numpy()\n",
    "        img_min, img_max = img.min(), img.max()\n",
    "        img = (img - img_min) / (img_max-img_min)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Predict labels using the model\n",
    "    predicted_labels = get_cifar10_outputs(dataset, model)\n",
    "    print(\"Labels predicted by the model:\")\n",
    "    print(predicted_labels)\n",
    "\n",
    "def get_cifar10_outputs(dataset, model):\n",
    "    outputs = []\n",
    "    for sample in dataset:\n",
    "        image = sample[0]\n",
    "        image = image.to(device)\n",
    "        if len(image.shape) < 4:\n",
    "            image = image.unsqueeze(0)\n",
    "        output = model(image)\n",
    "        predicted_label = torch.argmax(output, dim=1)\n",
    "        outputs.append(predicted_label.item())\n",
    "    return outputs\n",
    "    \n",
    "cifar10_lira_model = cifar10_lira_attack.model\n",
    "cifar10_trigger_model = cifar10_lira_attack.trigger_model\n",
    "poisoned_cifar10_testset = cifar10_lira_attack.get_poisoned_testset()\n",
    "\n",
    "num = 5\n",
    "cifar10_subset = [poisoned_cifar10_testset[i] for i in range(num*num)]\n",
    "\n",
    "class PoisonedLiraSubset(torch.utils.data.DataLoader):\n",
    "    def __init__(self, device, dataset, trigger, eps):\n",
    "        super().__init__(dataset)\n",
    "        self.device = device\n",
    "        self.trigger = trigger\n",
    "        self.eps = eps\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label, poisoned_label = self.dataset[idx]\n",
    "        img, trigger = img.to(device), self.trigger.to(device)\n",
    "        img += self.trigger(img.unsqueeze(0)).squeeze() * self.eps\n",
    "        return img, label, poisoned_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "transformed_cifar10_subset = PoisonedLiraSubset(device, cifar10_subset, cifar10_trigger_model, 0.1)\n",
    "    \n",
    "print('Normal Images')\n",
    "cifar10_plot(cifar10_subset, cifar10_lira_model, 5)\n",
    "print()\n",
    "print('Poisoned Images')\n",
    "cifar10_plot(transformed_cifar10_subset, cifar10_lira_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0c87e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c10_triggerlosses = cifar10_lira_attack.triggerlosses\n",
    "c10_classifierlosses = cifar10_lira_attack.classifierlosses\n",
    "c10_trainlosses = cifar10_lira_attack.trainlosses\n",
    "\n",
    "plot_losses(c10_triggerlosses, c10_classifierlosses, epochs*60000/128, c10_trainlosses, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebe640",
   "metadata": {},
   "outputs": [],
   "source": [
    "thismodel = cifar10_lira_attack.model\n",
    "thismodel = thismodel.to(device)\n",
    "cifar10_testloader = torch.utils.data.DataLoader(cifar10_testset, batch_size=150, shuffle=False, num_workers=10)\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in cifar10_testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = thismodel(images)\n",
    "        \n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += len(labels)\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c1273",
   "metadata": {},
   "source": [
    "## LIRA Attack on GTSRB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSRBAutoencoder(nn.Module):\n",
    "    \"\"\"The generator of backdoor trigger on GTSRB.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(GTSRBAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacd1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters based on the original paper\n",
    "gtsrb_epochs = 50\n",
    "gtsrb_finetune_epochs = 250\n",
    "gtsrb_eps = 0.005 # to preserve stealthiness of the data\n",
    "\n",
    "gtsrb_classifier = models.resnet18(num_classes=43).to(device)\n",
    "gtsrb_trigger = GTSRBAutoencoder().to(device)\n",
    "gtsrb_optimizer = optim.SGD(gtsrb_classifier.parameters(), lr=1e-2, momentum=0.9)\n",
    "gtsrb_finetune_optimizer = optim.SGD(gtsrb_classifier.parameters(), lr=1e-2, momentum=0.9)\n",
    "gtsrb_finetune_scheduler = optim.lr_scheduler.MultiStepLR(gtsrb_finetune_optimizer, milestones=[50,100,150,200,250], gamma=0.1)\n",
    "gtsrb_trigger_optimizer = optim.SGD(gtsrb_trigger.parameters(), lr=1e-4)\n",
    "\n",
    "gtsrb_lira_attack = LiraAttack(\n",
    "    device,\n",
    "    gtsrb_classifier,\n",
    "    gtsrb_trigger,\n",
    "    gtsrb_trainset,\n",
    "    gtsrb_testset,\n",
    "    target_class=1,\n",
    "    batch_size=128,\n",
    ")\n",
    "\n",
    "gtsrb_lira_attack.attack(\n",
    "    epochs=gtsrb_epochs,\n",
    "    finetune_epochs=gtsrb_finetune_epochs,\n",
    "    optimizer=gtsrb_optimizer,\n",
    "    finetune_optimizer=gtsrb_finetune_optimizer,\n",
    "    finetune_scheduler= gtsrb_finetune_scheduler,\n",
    "    trigger_optimizer=gtsrb_trigger_optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070eaaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backdoor",
   "language": "python",
   "name": "backdoor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
