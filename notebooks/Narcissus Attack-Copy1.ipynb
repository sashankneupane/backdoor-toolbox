{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28aff012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from backdoor.poisons import NarcissusPoison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7645d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1ecf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The argumention use for surrogate model training stage\n",
    "transform_surrogate_train = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# The argumention use for all training set\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# The argumention use for all testing set\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fb5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR10(root='/data/', train=True, download=False, transform=transform_train)\n",
    "testset = datasets.CIFAR10(root='/data/', train=False, download=False, transform=transform_test)\n",
    "pood_trainset = datasets.ImageFolder(root='/data/tiny-imagenet-200/train/', transform=transform_surrogate_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58772204",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_model = models.resnet18(num_classes=201).to(device)\n",
    "warmup_model = models.resnet18(num_classes=201).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073badbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = NarcissusPoison(device, pood_trainset, trainset, surrogate_model, warmup_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74469c7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sur_epochs = 200\n",
    "\n",
    "# sur_criterion = nn.CrossEntropyLoss()\n",
    "# sur_optimizer = optim.SGD\n",
    "# sur_scheduler = optim.lr_scheduler.CosineAnnealingLR\n",
    "\n",
    "# attack.train_surrogate(sur_epochs, sur_criterion, sur_optimizer, sur_scheduler)\n",
    "\n",
    "# surrogate already trained and is stored in './surrogate_model.pth'\n",
    "surrogate_model.load_state_dict(torch.load('./surrogate_model.pth'))\n",
    "attack.load_surrogate(surrogate_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e531a61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss: 5.183448e-01\n",
      "Epoch:1, Loss: 0.000000e+00\n",
      "Epoch:2, Loss: 0.000000e+00\n",
      "Epoch:3, Loss: 0.000000e+00\n",
      "Epoch:4, Loss: 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "warmup_epochs = 5\n",
    "warmup_criterion = nn.CrossEntropyLoss()\n",
    "warmup_optim = optim.RAdam\n",
    "\n",
    "model = attack.poi_warmup(warmup_epochs, warmup_criterion, warmup_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1bfe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RAdam\n",
    "\n",
    "attack.generate_trigger(1000, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = attack.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd062ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img, title=''):\n",
    "    if len(img.shape) > 3:\n",
    "        img = img[0]\n",
    "    img = np.moveaxis(img, 0, -1)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 23\n",
    "\n",
    "noised_img = trainset[idx][0].numpy() + noise[0]\n",
    "print(noised_img.shape)\n",
    "\n",
    "plot_img(noise, 'Noise')\n",
    "plot_img(trainset[idx][0].numpy(), 'Normal Image')\n",
    "plot_img(noised_img, 'Noised Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./noise.npy', noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "poison_amount = 25\n",
    "\n",
    "noise_testing_model = models.resnet18(num_classes=10)\n",
    "noise_testing_model = noise_testing_model.to(device)\n",
    "\n",
    "train_epochs = 200\n",
    "train_lr = 0.1\n",
    "test_batch_size = 150\n",
    "\n",
    "multi_test = 3\n",
    "random_seed = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "model = noise_testing_model\n",
    "\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=train_lr, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f54e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 0\n",
    "train_target_list = list(np.where(np.array(trainset.targets) == target_class)[0])\n",
    "\n",
    "transform_after_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get poisoned dataset for training the model\n",
    "class PoisonedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, noise, transform):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.noise = noise\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if idx in self.indices:\n",
    "            image += self.noise\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return (image, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "random_poison_idx = random.sample(train_target_list, poison_amount)\n",
    "poison_train_target = PoisonedDataset(trainset, random_poison_idx, noise[0], transform_after_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc64a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Traing dataset size is:',len(poison_train_target),\" Poison numbers is:\",len(random_poison_idx))\n",
    "clean_train_loader = torch.utils.data.DataLoader(poison_train_target, batch_size=test_batch_size, shuffle=True, num_workers=5)\n",
    "clean_test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=False, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded77404",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_non_target = list(np.where(np.array(testset.targets)!= target_class)[0])\n",
    "\n",
    "class AsrDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, indices, taget_class, noise, magnify):\n",
    "        self.dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        self.target_class = target_class\n",
    "        self.noise = noise\n",
    "        if len(noise.shape) > 3:\n",
    "            self.noise = noise[0]\n",
    "        self.magnify = magnify\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.dataset[idx][0]\n",
    "        img += self.noise * self.magnify\n",
    "        return (img, self.target_class)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "test_target_indices = list(np.where(np.array(testset.targets) == target_class)[0])\n",
    "test_target_dataset = torch.utils.data.Subset(testset, test_target_indices)\n",
    "\n",
    "asr_testset = AsrDataset(testset, test_non_target, 0, noise, 3)\n",
    "\n",
    "asr_loader = torch.utils.data.DataLoader(asr_testset, batch_size=test_batch_size, shuffle=False, num_workers=5)\n",
    "target_test_loader = torch.utils.data.DataLoader(test_target_dataset, batch_size=test_batch_size, shuffle=False, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14c423",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(train_epochs):\n",
    "    \n",
    "    print(f'Epoch: [{epoch+1}/{train_epochs}]')\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for images, labels in clean_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        train_losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Train Loss: {sum(train_losses)/len(train_losses)}')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get clean test accuracy\n",
    "    correct_clean, total_clean = 0, 0\n",
    "    for i, (images, labels) in enumerate(clean_test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total_clean += labels.size(0)\n",
    "            correct_clean += (predicted == labels).sum().item()\n",
    "    acc_clean = correct_clean / total_clean\n",
    "    print('Clean Test Accuracy %.2f' % (acc_clean))\n",
    "    \n",
    "    # Get target clean accuracy\n",
    "    correct_tar, total_tar = 0, 0\n",
    "    for i, (images, labels) in enumerate(target_test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total_tar += labels.size(0)\n",
    "            correct_tar += (predicted == labels).sum().item()\n",
    "    acc_tar = correct_tar / total_tar\n",
    "    print('\\nTarget Test Clean Accuracy %.2f' % (acc_tar))\n",
    "\n",
    "    # Get Attack Success Rate\n",
    "    correct, total = 0, 0\n",
    "    for i, (images, labels) in enumerate(asr_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = correct/total\n",
    "    print(f'Attack Success Rate: {acc}')\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76a4af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = 80\n",
    "magnify = 1\n",
    "\n",
    "img = testset[idx][0]\n",
    "\n",
    "plot_img(img.numpy())\n",
    "plot_img(img.numpy() + noise[0] * magnify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2261eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(model, idx, noise=None, magnify=magnify):\n",
    "    img = testset[idx][0]\n",
    "    img = img.unsqueeze(dim=0)\n",
    "    if noise is not None:\n",
    "        img += noise\n",
    "    logits = model(img.to(device))\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "    return predicted.item(), testset[idx][1]\n",
    "\n",
    "output(model, idx, noise, magnify)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backdoor",
   "language": "python",
   "name": "backdoor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
